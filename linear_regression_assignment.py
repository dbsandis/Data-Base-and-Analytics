# -*- coding: utf-8 -*-
"""Copy of Linear_Regression_Assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SMsch7x4_j3fOsvDFUqODKiVOJz7V-bQ

# **DSBA/MBAD 6201 Assignment:** Data Exploration and Multiple Linear Regression

# Load Data
"""

import os
print(os.listdir())

import pandas as pd #import package
df = pd.read_csv('auto_mpg_edit.csv') #load the csv file

df.head()

df.info()

df.describe()

"""# Data Preprocessing"""

df.drop(columns=['car name'], inplace=True) # drop the 'car name' column because it is not useful

"""## 1a. Generate box-plot for the horsepower and acceleration attributes and identify the cutoff values for outliers. (2 pts)"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Setting the seaborn style for better aesthetics
sns.set(style="whitegrid")

# Creating box plots for horsepower and acceleration
fig, axs = plt.subplots(1, 2, figsize=(12, 6))

# Horsepower box plot
sns.boxplot(data=df, y="horsepower", ax=axs[0])
axs[0].set_title('Box plot of Horsepower')

# Acceleration box plot
sns.boxplot(data=df, y="acceleration", ax=axs[1])
axs[1].set_title('Box plot of Acceleration')

plt.tight_layout()
plt.show()

# Calculate and print the cutoff values for outliers for both horsepower and acceleration

# Horsepower
Q1_hp, Q3_hp = df['horsepower'].quantile([0.25, 0.75])
IQR_hp = Q3_hp - Q1_hp
outlier_low_hp = Q1_hp - 1.5 * IQR_hp
outlier_high_hp = Q3_hp + 1.5 * IQR_hp

# Acceleration
Q1_acc, Q3_acc = df['acceleration'].quantile([0.25, 0.75])
IQR_acc = Q3_acc - Q1_acc
outlier_low_acc = Q1_acc - 1.5 * IQR_acc
outlier_high_acc = Q3_acc + 1.5 * IQR_acc

print(f'Horsepower outliers are below {outlier_low_hp} or above {outlier_high_hp}.')
print(f'Acceleration outliers are below {outlier_low_acc} or above {outlier_high_acc}.')

"""## 1b. Genrate a scatterplot for acceleration against horsepower. (2 pts)"""

# Generate a scatterplot for acceleration against horsepower
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df, x='horsepower', y='acceleration')
plt.title('Scatterplot of Acceleration vs Horsepower')
plt.xlabel('Horsepower')
plt.ylabel('Acceleration')
plt.show()

"""## 1c. Comment on how inclusion of the outliers would affect a predictive model of 'mpg' as a function of 'acceleration'. (2 pt)

Inclusion of outliers in a predictive model of 'mpg' as a function of 'acceleration' can significantly impact its accuracy and reliability. Outliers can skew the model, leading to increased prediction errors, reduced performance on unseen data, and potential biases in the estimates.

## 2. 'mpg' has a somewhat longish tail and is not precisely normally distributed, so we will take a log transformation, ( use df['lmpg'] = df['mpg'].apply(np.log) ), and then predict 'lmpg' instead. (You should convince yourself that this is a better idea by looking at the histograms to assess normality; however no need to submit such plots.) (2 pt)
"""

import numpy as np
import pandas as pd

df['lmpg'] = df['mpg'].apply(np.log)
import matplotlib.pyplot as plt
fig, axs = plt.subplots(1, 2, figsize=(12, 6))
df['mpg'].hist(ax=axs[0], bins=20)
axs[0].set_title('Histogram of mpg')
df['lmpg'].hist(ax=axs[1], bins=20)
axs[1].set_title('Histogram of log-transformed mpg (lmpg)')
from sklearn.model_selection import train_test_split

plt.show()

"""Applying the logarithm compresses the data and helps ignore the outliers
closer to a normal distribution.

## Split the data into train and test sets
"""

# Specify the independent variables (X) and dependent variable (Y)
X = df[['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration']]
y = df['lmpg']

seed = 0  # Assign a seed value.

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)

"""# Regression analysis and assesment

## 3. Try to fit an MLR to this dataset, with 'lmpg' as the dependent variable. Use all the available variables in your model. (4 pts)
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Apply the log transformation to 'mpg' to create 'lmpg'
df['lmpg'] = np.log(df['mpg'])

# Define the independent variables (X) and the dependent variable (y)
# Here we're using all available variables as predictors except 'mpg' itself and 'car name'
X = df[['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration']]
y = df['lmpg']

# Split the dataset into training and testing sets
seed = 0  # Seed for reproducibility
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)

# Initialize and fit the Multiple Linear Regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the testing set
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Print the performance metrics
print(f'Mean Squared Error (MSE): {mse}')
print(f'R-squared Value: {r2}')

"""## 4. Report the coefficients obtained by your model. Would you drop any of the variables used in your model (based on the t-scores or p-values)? (5 pts)

"""

import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.feature_selection import f_regression
from sklearn.model_selection import train_test_split

# Assuming 'df' is your DataFrame after log transformation has been applied
X = df[['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration']]
y = df['lmpg']

# Splitting the data (Assuming this has not been done in the current context)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Fitting the MLR model
model = LinearRegression()
model.fit(X_train, y_train)

# Obtaining coefficients
coefficients = model.coef_

# Calculating F-statistics and p-values
F, p_values = f_regression(X_train, y_train)

# Compiling the results into a DataFrame
results_df = pd.DataFrame({
    'Variable': X.columns,
    'Coefficient': coefficients,
    'F-Statistic': F,
    'p-value': p_values
})

print(results_df)

"""## 5. Report the MSE obtained on X_train. Score your model (i.e., predict) on X_test. Also report how much the MSE changes. (3 pts)

"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Log-transform 'mpg' and define predictors and the response variable
df['lmpg'] = np.log(df['mpg'])
X = df[['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration']]
y = df['lmpg']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Initialize and fit the Linear Regression model to the training data
model = LinearRegression()
model.fit(X_train, y_train)

# Predict on the training data and calculate MSE
y_train_pred = model.predict(X_train)
mse_train = mean_squared_error(y_train, y_train_pred)

# Predict on the testing data and calculate MSE
y_test_pred = model.predict(X_test)
mse_test = mean_squared_error(y_test, y_test_pred)

# Calculate the change in MSE
mse_change = mse_test - mse_train

# Report the MSE for the training set, testing set, and the change in MSE
print(f"MSE on training set: {mse_train}")
print(f"MSE on testing set: {mse_test}")
print(f"Change in MSE from training to testing: {mse_change}")

"""## 6. Bonus Question. Use the stepwise regression to reach your final model. Try different model selection criteria (i.e., AIC, BIC, Adj R^2) and see if you can come up with the same model even with the different criteria. Determine the best model if you get different models with different criteria. (Consider a model that gives the lowest MSE on the test set as the best model) (2 pts)."""

